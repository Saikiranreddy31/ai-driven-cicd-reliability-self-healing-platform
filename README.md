# ai-driven-cicd-reliability-self-healing-platform
An AI-driven CI/CD reliability engineering platform that detects pipeline and infrastructure failures, analyzes root causes, and performs automated self-healing actions with observability and rollback support.

This project is a hands-on SRE / DevOps system built to simulate real-world reliability problems and show how modern teams observe, alert, and respond to failures.

Instead of a toy app, this setup behaves like a real production service: it exposes metrics, fails in controlled ways, triggers alerts, and visualizes everything through dashboards. The next step is to make it self-healing.

ğŸ¯ What Iâ€™m Building (and Why)

The goal of this project is to understand and demonstrate how reliability is handled in real systems, not just how apps are deployed.


This project focuses on:

Building a production-like microservice

Adding metrics from day one

Monitoring the service using Prometheus

Visualizing system health in Grafana

Defining SLOs and error budgets

Triggering alerts when things go wrong

(Next) Automatically fixing issues using self-healing automation

ğŸ§± High-Level Architecture
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Sample App â”‚  (FastAPI + Failure Injection)
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
      â”‚ /metrics
â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prometheus   â”‚  (Metrics scraping & recording rules)
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Grafana      â”‚  (Dashboards & visualization)
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Alertmanager â”‚  (SLO-based alerts)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“¦ Tech Stack

Python (FastAPI) â€“ service implementation

Prometheus â€“ metrics collection & rules

Grafana â€“ dashboards and visualization

Alertmanager â€“ alert routing

Docker & Docker Compose â€“ containerization

Failure Injection (Chaos) â€“ controlled outages

GitHub Actions (Phase 4) â€“ CI/CD & automation

ğŸš€ Phase-Wise Implementation
âœ… Phase 1 â€” Sample Service & Failure Injection

What I built:

A FastAPI service with:

/ping

/health

/metrics

Built-in failure modes:

latency (slow responses)

crash (process exits)

error (500 errors)

memory (simulated memory leak)

Failures are controlled using an environment variable: FAIL_MODE

Why this matters:

Real reliability work starts by expecting failures, not avoiding them.

âœ… Phase 2 â€” Dockerization

What I did:

Dockerized the application

Standardized runtime using Uvicorn

Verified the app runs the same locally and inside Docker

Outcome:

The service is now portable, reproducible, and production-ready.

âœ… Phase 3 â€” Observability & Reliability Engineering
ğŸ”¹ Phase 3.1 â€” Metrics Instrumentation

Added custom Prometheus metrics:

Request count

Request latency (histograms)

ğŸ”¹ Phase 3.2 â€” Prometheus Setup

Configured Prometheus scraping

Docker-based service discovery

Verified targets and metrics ingestion

ğŸ”¹ Phase 3.3 â€” Grafana Dashboards

Created and provisioned dashboards for:

Request rate (RPS)

Latency (P95)

Service availability

ğŸ”¹ Phase 3.4 â€” SLOs & Alerting

Defined recording rules:

sample_app:request_rate

sample_app:latency_p95

sample_app:availability

Configured Alertmanager

Alerts fire automatically when SLOs are breached

Verified end-to-end:

Prometheus shows alert states

Grafana reflects real-time spikes

Alertmanager receives and manages alerts

ğŸ§ª Failure Testing

Run the service with failures enabled:

docker run -e FAIL_MODE=latency -p 8000:8000 failing-service


or:

docker run -e FAIL_MODE=crash -p 8000:8000 failing-service


What youâ€™ll see:

ğŸš¨ Alerts firing in Prometheus

ğŸ“‰ Latency spikes in Grafana

ğŸ“Š Dashboards updating in real time

ğŸ Current Status

âœ” Phase 1 â€” Completed

âœ” Phase 2 â€” Completed

âœ” Phase 3 â€” Completed